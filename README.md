# ViT-Height

In this project, I drew inspiration from several classic networks such as VGG, ResNet, DarkNet, and Transformer. I adapted these networks using multi-modal data (grayscale and depth) to investigate pedestrian height regression.

I modified the popular multi-modal learning model ViLT (Vision-and-Language Transformer) by transforming its original input channels (text and image) into multi-modal images (depth and grayscale). Additionally, I fine-tuned its output for height regression, and I named this modified version ViT-Height.

Please click [here](https://rachelxhoot.com/ViT-Height-939d9f7563984179bdfd8399b1d0e5ab) for more details!

![](./ViT-Height.png)
